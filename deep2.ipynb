{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import torch\n",
    "from collections import deque\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "env = gym.envs.make(\"MountainCar-v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, n_state, n_action, n_hidden = 50, lr= 0.05):\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_state, n_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_hidden, n_action)\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),lr\n",
    "        )\n",
    "    def update(self, s,y):\n",
    "        \"\"\"\n",
    "        Update the weights of the DQN given a training sample\n",
    "        @param s : state\n",
    "        @param y: target value\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = self.model(torch.Tensor(s))\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def predict(self, s ):\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(s))\n",
    "\n",
    "    def replay(self, memory, replay_size, gamma):\n",
    "        if len(memory) >= replay_size:\n",
    "            replay_data = random.sample(memory, replay_size)\n",
    "            states =[]\n",
    "            td_targets =[]\n",
    "            for state,action, next_state, reward, is_done in replay_data:\n",
    "                print(\"state\",state)\n",
    "                states.append(state)\n",
    "                q_values = self.predict(state).tolist()\n",
    "                if is_done:\n",
    "                    q_values[action] = reward\n",
    "                else:\n",
    "                    q_values_next = self.predict(next_state)\n",
    "                    q_values[action] = reward + gamma*torch.max(q_values_next).item()\n",
    "\n",
    "                td_targets.append(q_values)\n",
    "\n",
    "            self.update(states, td_targets)\n",
    "\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_epsilon_greedy_policy(estimator, epsilon , n_action):\n",
    "    def policy_function(state):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, n_action -1 )\n",
    "\n",
    "        else :\n",
    "            q_values = estimator.predict(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "    return policy_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "n_hidden = 50\n",
    "lr = 0.001\n",
    "dqn = DQN(n_state, n_action , n_hidden, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, estimator, n_episode , replay_size, gamma= 1.0,epsilon=0.1, epsilon_decay =0.99):\n",
    "    for episode in range(n_episode):\n",
    "        policy = gen_epsilon_greedy_policy(\n",
    "            estimator, epsilon, n_action\n",
    "        )\n",
    "        state,_ = env.reset()\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, is_done ,_, _ = env.step(action)\n",
    "            total_reward_episode[episode] += reward\n",
    "            modififed_reward = next_state[0] + 0.5\n",
    "            if next_state[0] >= 0.5:\n",
    "                modififed_reward +=100\n",
    "\n",
    "            elif next_state[0] >= 0.25:\n",
    "                modififed_reward +=20\n",
    "\n",
    "            elif next_state[0] >= 0.1:\n",
    "                modififed_reward +=10\n",
    "\n",
    "            elif next_state[0] >= 0: \n",
    "                modififed_reward += 5\n",
    "\n",
    "            memory.append((state,action,next_state, modififed_reward,  is_done))\n",
    "\n",
    "            if is_done :\n",
    "                break\n",
    "                \n",
    "            estimator.replay(memory, replay_size, gamma)\n",
    "            state = next_state\n",
    "\n",
    "        print('Episode:{} , total reward:{}, epsilon: {}'.format(episode, total_reward_episode[episode],epsilon))\n",
    "\n",
    "        epsilon = max(epsilon* epsilon_decay, 0.01)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward_episode = [0]*n_episode\n",
    "q_learning(env, dqn,n_episode, replay_size, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(total_reward_episode)\n",
    "plt.title('episdoe reward over time ')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('total reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
