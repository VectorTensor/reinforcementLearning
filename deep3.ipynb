{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import torch\n",
    "from collections import deque\n",
    "import random \n",
    "import copy \n",
    "from torch.autograd import Variable\n",
    "env = gym.envs.make(\"MountainCar-v0\")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, n_state,n_action,  n_hidden=50, lr=0.05):\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_state, n_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_hidden, n_action)\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "\n",
    "        self.model_target = copy.deepcopy(self.model)\n",
    "\n",
    "    def target_predict(self, s):\n",
    "        with torch.no_grad():\n",
    "            return self.model_target(torch.Tensor(s))\n",
    "\n",
    "    def copy_target(self):\n",
    "        self.model_target.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def replay(self, memory, replay_size, gamma ):\n",
    "        if len(memory) >= replay_size:\n",
    "            replay_data = random.sample(memory, replay_size)\n",
    "            states= []\n",
    "            td_targets =[]\n",
    "            for state, action, next_state, reward, is_done in replay_data:\n",
    "                states.append(state)\n",
    "                q_values = self.predict(state).tolist()\n",
    "                if is_done:\n",
    "                    q_values[action]= reward\n",
    "                else:\n",
    "                    q_values_next = self.target_predict(next_state).detach()\n",
    "                    q_values[action] = reward + gamma*torch.max(q_values_next).item()\n",
    "\n",
    "                td_targets.append(q_values)\n",
    "            \n",
    "            states = np.array(states)\n",
    "            self.update(states, td_targets)\n",
    "    def update(self, s,y):\n",
    "        \"\"\"\n",
    "        Update the weights of the DQN given a training sample\n",
    "        @param s : state\n",
    "        @param y: target value\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = self.model(torch.Tensor(s))\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def predict(self, s ):\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_epsilon_greedy_policy(estimator, epsilon , n_action):\n",
    "    def policy_function(state):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, n_action -1 )\n",
    "\n",
    "        else :\n",
    "            q_values = estimator.predict(state)\n",
    "            return torch.argmax(q_values).item()\n",
    " \n",
    "    return policy_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "n_hidden = 50 \n",
    "lr = 0.01\n",
    "dqn = DQN(n_state, n_action, n_hidden, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, estimator, n_episode, replay_size, target_update=10, gamma=1.0 , epsilon=0.1, epsilon_decay=0.99):\n",
    "    for episode in range(n_episode):\n",
    "        if episode % target_update ==0:\n",
    "            estimator.copy_target()\n",
    "        policy = gen_epsilon_greedy_policy(\n",
    "            estimator, epsilon , n_action\n",
    "        )\n",
    "        state,_ = env.reset()\n",
    "\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            action = policy(state)\n",
    "            next_state , reward, is_done , _ , _ = env.step(action)\n",
    "            total_reward_episode[episode] += reward\n",
    "            modified_reward = next_state[0] + 0.5\n",
    "            if next_state[0]  >= 0.5:\n",
    "                modified_reward += 100\n",
    "            elif next_state[0] >= 0.25:\n",
    "                modified_reward +=20\n",
    "\n",
    "            elif next_state[0] >= 0.1:\n",
    "                modified_reward +=10\n",
    "\n",
    "            elif next_state[0] >= 0: \n",
    "                modified_reward += 5\n",
    "\n",
    "            memory.append((state,action,next_state, modified_reward,  is_done))\n",
    "\n",
    "            if is_done :\n",
    "                break\n",
    "                \n",
    "            estimator.replay(memory, replay_size, gamma)\n",
    "            state = next_state\n",
    "        print('episode: {}, total reward :{}, epsilon:{}'.format(episode, total_reward_episode[episode],\n",
    "        epsilon))\n",
    "\n",
    "        epsilon = max(epsilon * epsilon_decay, 0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 1000\n",
    "replay_size = 20\n",
    "target_update =10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, total reward :-5924.0, epsilon:1\n",
      "episode: 1, total reward :-30215.0, epsilon:0.99\n",
      "episode: 2, total reward :-18757.0, epsilon:0.9801\n",
      "episode: 3, total reward :-16222.0, epsilon:0.9702989999999999\n",
      "episode: 4, total reward :-14152.0, epsilon:0.96059601\n",
      "episode: 5, total reward :-15230.0, epsilon:0.9509900498999999\n",
      "episode: 6, total reward :-10591.0, epsilon:0.9414801494009999\n",
      "episode: 7, total reward :-4586.0, epsilon:0.9320653479069899\n",
      "episode: 8, total reward :-2231.0, epsilon:0.92274469442792\n",
      "episode: 9, total reward :-29053.0, epsilon:0.9135172474836407\n",
      "episode: 10, total reward :-13215.0, epsilon:0.9043820750088043\n",
      "episode: 11, total reward :-11415.0, epsilon:0.8953382542587163\n",
      "episode: 12, total reward :-13694.0, epsilon:0.8863848717161291\n",
      "episode: 13, total reward :-14292.0, epsilon:0.8775210229989678\n",
      "episode: 14, total reward :-3031.0, epsilon:0.8687458127689781\n",
      "episode: 15, total reward :-9912.0, epsilon:0.8600583546412883\n",
      "episode: 16, total reward :-12116.0, epsilon:0.8514577710948754\n",
      "episode: 17, total reward :-7828.0, epsilon:0.8429431933839266\n",
      "episode: 18, total reward :-2394.0, epsilon:0.8345137614500874\n",
      "episode: 19, total reward :-2745.0, epsilon:0.8261686238355865\n",
      "episode: 20, total reward :-8009.0, epsilon:0.8179069375972307\n",
      "episode: 21, total reward :-3500.0, epsilon:0.8097278682212583\n",
      "episode: 22, total reward :-2055.0, epsilon:0.8016305895390458\n",
      "episode: 23, total reward :-3851.0, epsilon:0.7936142836436553\n",
      "episode: 24, total reward :-1770.0, epsilon:0.7856781408072188\n",
      "episode: 25, total reward :-2100.0, epsilon:0.7778213593991465\n",
      "episode: 26, total reward :-6359.0, epsilon:0.7700431458051551\n",
      "episode: 27, total reward :-14658.0, epsilon:0.7623427143471035\n",
      "episode: 28, total reward :-1851.0, epsilon:0.7547192872036325\n",
      "episode: 29, total reward :-3712.0, epsilon:0.7471720943315961\n",
      "episode: 30, total reward :-735.0, epsilon:0.7397003733882802\n",
      "episode: 31, total reward :-5484.0, epsilon:0.7323033696543974\n",
      "episode: 32, total reward :-4917.0, epsilon:0.7249803359578534\n",
      "episode: 33, total reward :-4237.0, epsilon:0.7177305325982748\n",
      "episode: 34, total reward :-2916.0, epsilon:0.7105532272722921\n",
      "episode: 35, total reward :-2406.0, epsilon:0.7034476949995692\n",
      "episode: 36, total reward :-1094.0, epsilon:0.6964132180495735\n",
      "episode: 37, total reward :-4445.0, epsilon:0.6894490858690777\n",
      "episode: 38, total reward :-1847.0, epsilon:0.682554595010387\n",
      "episode: 39, total reward :-2838.0, epsilon:0.6757290490602831\n",
      "episode: 40, total reward :-1277.0, epsilon:0.6689717585696803\n",
      "episode: 41, total reward :-1219.0, epsilon:0.6622820409839835\n",
      "episode: 42, total reward :-2989.0, epsilon:0.6556592205741436\n",
      "episode: 43, total reward :-2823.0, epsilon:0.6491026283684022\n",
      "episode: 44, total reward :-1324.0, epsilon:0.6426116020847181\n",
      "episode: 45, total reward :-725.0, epsilon:0.6361854860638709\n",
      "episode: 46, total reward :-1759.0, epsilon:0.6298236312032323\n",
      "episode: 47, total reward :-2748.0, epsilon:0.6235253948912\n",
      "episode: 48, total reward :-2000.0, epsilon:0.617290140942288\n",
      "episode: 49, total reward :-2321.0, epsilon:0.6111172395328651\n",
      "episode: 50, total reward :-1158.0, epsilon:0.6050060671375365\n",
      "episode: 51, total reward :-483.0, epsilon:0.5989560064661611\n",
      "episode: 52, total reward :-1078.0, epsilon:0.5929664464014994\n",
      "episode: 53, total reward :-2141.0, epsilon:0.5870367819374844\n",
      "episode: 54, total reward :-2336.0, epsilon:0.5811664141181095\n",
      "episode: 55, total reward :-2494.0, epsilon:0.5753547499769285\n",
      "episode: 56, total reward :-2675.0, epsilon:0.5696012024771592\n",
      "episode: 57, total reward :-644.0, epsilon:0.5639051904523876\n",
      "episode: 58, total reward :-576.0, epsilon:0.5582661385478638\n",
      "episode: 59, total reward :-2566.0, epsilon:0.5526834771623851\n",
      "episode: 60, total reward :-799.0, epsilon:0.5471566423907612\n",
      "episode: 61, total reward :-1348.0, epsilon:0.5416850759668536\n",
      "episode: 62, total reward :-1591.0, epsilon:0.536268225207185\n",
      "episode: 63, total reward :-1132.0, epsilon:0.5309055429551132\n",
      "episode: 64, total reward :-814.0, epsilon:0.525596487525562\n",
      "episode: 65, total reward :-498.0, epsilon:0.5203405226503064\n",
      "episode: 66, total reward :-500.0, epsilon:0.5151371174238033\n",
      "episode: 67, total reward :-951.0, epsilon:0.5099857462495653\n",
      "episode: 68, total reward :-1101.0, epsilon:0.5048858887870696\n",
      "episode: 69, total reward :-905.0, epsilon:0.4998370298991989\n",
      "episode: 70, total reward :-1513.0, epsilon:0.49483865960020695\n",
      "episode: 71, total reward :-1936.0, epsilon:0.4898902730042049\n",
      "episode: 72, total reward :-1847.0, epsilon:0.48499137027416284\n",
      "episode: 73, total reward :-795.0, epsilon:0.4801414565714212\n",
      "episode: 74, total reward :-831.0, epsilon:0.475340042005707\n",
      "episode: 75, total reward :-669.0, epsilon:0.47058664158564995\n",
      "episode: 76, total reward :-1594.0, epsilon:0.4658807751697934\n",
      "episode: 77, total reward :-569.0, epsilon:0.4612219674180955\n",
      "episode: 78, total reward :-817.0, epsilon:0.45660974774391455\n",
      "episode: 79, total reward :-950.0, epsilon:0.4520436502664754\n",
      "episode: 80, total reward :-2297.0, epsilon:0.44752321376381066\n",
      "episode: 81, total reward :-586.0, epsilon:0.44304798162617254\n",
      "episode: 82, total reward :-847.0, epsilon:0.4386175018099108\n",
      "episode: 83, total reward :-1020.0, epsilon:0.4342313267918117\n",
      "episode: 84, total reward :-1001.0, epsilon:0.4298890135238936\n",
      "episode: 85, total reward :-917.0, epsilon:0.42559012338865465\n",
      "episode: 86, total reward :-1039.0, epsilon:0.4213342221547681\n",
      "episode: 87, total reward :-910.0, epsilon:0.41712087993322045\n",
      "episode: 88, total reward :-703.0, epsilon:0.41294967113388825\n",
      "episode: 89, total reward :-1210.0, epsilon:0.40882017442254937\n",
      "episode: 90, total reward :-293.0, epsilon:0.4047319726783239\n",
      "episode: 91, total reward :-3479.0, epsilon:0.40068465295154065\n",
      "episode: 92, total reward :-2722.0, epsilon:0.39667780642202527\n",
      "episode: 93, total reward :-1601.0, epsilon:0.392711028357805\n",
      "episode: 94, total reward :-1189.0, epsilon:0.38878391807422696\n",
      "episode: 95, total reward :-1981.0, epsilon:0.3848960788934847\n",
      "episode: 96, total reward :-357.0, epsilon:0.38104711810454983\n",
      "episode: 97, total reward :-969.0, epsilon:0.37723664692350434\n",
      "episode: 98, total reward :-713.0, epsilon:0.37346428045426927\n",
      "episode: 99, total reward :-3828.0, epsilon:0.36972963764972655\n",
      "episode: 100, total reward :-744.0, epsilon:0.36603234127322926\n",
      "episode: 101, total reward :-556.0, epsilon:0.36237201786049694\n",
      "episode: 102, total reward :-1813.0, epsilon:0.358748297681892\n",
      "episode: 103, total reward :-383.0, epsilon:0.35516081470507305\n",
      "episode: 104, total reward :-1652.0, epsilon:0.3516092065580223\n",
      "episode: 105, total reward :-622.0, epsilon:0.34809311449244207\n",
      "episode: 106, total reward :-342.0, epsilon:0.34461218334751764\n",
      "episode: 107, total reward :-719.0, epsilon:0.34116606151404244\n",
      "episode: 108, total reward :-693.0, epsilon:0.337754400898902\n",
      "episode: 109, total reward :-867.0, epsilon:0.334376856889913\n",
      "episode: 110, total reward :-475.0, epsilon:0.33103308832101386\n",
      "episode: 111, total reward :-821.0, epsilon:0.3277227574378037\n",
      "episode: 112, total reward :-809.0, epsilon:0.3244455298634257\n",
      "episode: 113, total reward :-1433.0, epsilon:0.3212010745647914\n",
      "episode: 114, total reward :-518.0, epsilon:0.3179890638191435\n",
      "episode: 115, total reward :-1287.0, epsilon:0.31480917318095203\n",
      "episode: 116, total reward :-568.0, epsilon:0.3116610814491425\n",
      "episode: 117, total reward :-1356.0, epsilon:0.30854447063465107\n",
      "episode: 118, total reward :-674.0, epsilon:0.30545902592830454\n",
      "episode: 119, total reward :-399.0, epsilon:0.3024044356690215\n",
      "episode: 120, total reward :-821.0, epsilon:0.29938039131233124\n",
      "episode: 121, total reward :-691.0, epsilon:0.2963865873992079\n",
      "episode: 122, total reward :-668.0, epsilon:0.29342272152521587\n",
      "episode: 123, total reward :-626.0, epsilon:0.2904884943099637\n",
      "episode: 124, total reward :-458.0, epsilon:0.28758360936686406\n",
      "episode: 125, total reward :-1209.0, epsilon:0.2847077732731954\n",
      "episode: 126, total reward :-279.0, epsilon:0.28186069554046345\n",
      "episode: 127, total reward :-728.0, epsilon:0.2790420885850588\n",
      "episode: 128, total reward :-240.0, epsilon:0.2762516676992082\n",
      "episode: 129, total reward :-350.0, epsilon:0.27348915102221616\n",
      "episode: 130, total reward :-1524.0, epsilon:0.270754259511994\n",
      "episode: 131, total reward :-478.0, epsilon:0.26804671691687404\n",
      "episode: 132, total reward :-768.0, epsilon:0.2653662497477053\n",
      "episode: 133, total reward :-1061.0, epsilon:0.2627125872502282\n",
      "episode: 134, total reward :-618.0, epsilon:0.2600854613777259\n",
      "episode: 135, total reward :-1697.0, epsilon:0.2574846067639487\n",
      "episode: 136, total reward :-953.0, epsilon:0.2549097606963092\n",
      "episode: 137, total reward :-718.0, epsilon:0.2523606630893461\n",
      "episode: 138, total reward :-568.0, epsilon:0.24983705645845267\n",
      "episode: 139, total reward :-315.0, epsilon:0.24733868589386815\n",
      "episode: 140, total reward :-585.0, epsilon:0.24486529903492946\n",
      "episode: 141, total reward :-397.0, epsilon:0.24241664604458016\n",
      "episode: 142, total reward :-201.0, epsilon:0.23999247958413436\n",
      "episode: 143, total reward :-325.0, epsilon:0.23759255478829303\n",
      "episode: 144, total reward :-649.0, epsilon:0.2352166292404101\n",
      "episode: 145, total reward :-215.0, epsilon:0.232864462948006\n",
      "episode: 146, total reward :-593.0, epsilon:0.23053581831852593\n",
      "episode: 147, total reward :-919.0, epsilon:0.22823046013534068\n",
      "episode: 148, total reward :-1469.0, epsilon:0.22594815553398728\n",
      "episode: 149, total reward :-1192.0, epsilon:0.22368867397864742\n",
      "episode: 150, total reward :-1020.0, epsilon:0.22145178723886094\n",
      "episode: 151, total reward :-467.0, epsilon:0.21923726936647234\n",
      "episode: 152, total reward :-707.0, epsilon:0.2170448966728076\n",
      "episode: 153, total reward :-1362.0, epsilon:0.21487444770607952\n",
      "episode: 154, total reward :-2860.0, epsilon:0.21272570322901874\n",
      "episode: 155, total reward :-865.0, epsilon:0.21059844619672854\n",
      "episode: 156, total reward :-637.0, epsilon:0.20849246173476127\n",
      "episode: 157, total reward :-600.0, epsilon:0.20640753711741366\n",
      "episode: 158, total reward :-394.0, epsilon:0.20434346174623952\n",
      "episode: 159, total reward :-2555.0, epsilon:0.20230002712877712\n",
      "episode: 160, total reward :-356.0, epsilon:0.20027702685748935\n",
      "episode: 161, total reward :-311.0, epsilon:0.19827425658891445\n",
      "episode: 162, total reward :-1027.0, epsilon:0.1962915140230253\n",
      "episode: 163, total reward :-997.0, epsilon:0.19432859888279505\n",
      "episode: 164, total reward :-3064.0, epsilon:0.1923853128939671\n",
      "episode: 165, total reward :-310.0, epsilon:0.19046145976502743\n",
      "episode: 166, total reward :-864.0, epsilon:0.18855684516737714\n",
      "episode: 167, total reward :-1690.0, epsilon:0.18667127671570335\n",
      "episode: 168, total reward :-543.0, epsilon:0.18480456394854633\n",
      "episode: 169, total reward :-350.0, epsilon:0.18295651830906087\n",
      "episode: 170, total reward :-476.0, epsilon:0.18112695312597027\n",
      "episode: 171, total reward :-554.0, epsilon:0.17931568359471056\n",
      "episode: 172, total reward :-396.0, epsilon:0.17752252675876345\n",
      "episode: 173, total reward :-811.0, epsilon:0.17574730149117582\n",
      "episode: 174, total reward :-959.0, epsilon:0.17398982847626407\n",
      "episode: 175, total reward :-1486.0, epsilon:0.17224993019150142\n",
      "episode: 176, total reward :-1523.0, epsilon:0.1705274308895864\n",
      "episode: 177, total reward :-1357.0, epsilon:0.16882215658069055\n",
      "episode: 178, total reward :-1381.0, epsilon:0.16713393501488363\n",
      "episode: 179, total reward :-1510.0, epsilon:0.16546259566473479\n",
      "episode: 180, total reward :-511.0, epsilon:0.16380796970808745\n",
      "episode: 181, total reward :-355.0, epsilon:0.16216989001100657\n",
      "episode: 182, total reward :-1158.0, epsilon:0.1605481911108965\n",
      "episode: 183, total reward :-2896.0, epsilon:0.15894270919978754\n",
      "episode: 184, total reward :-855.0, epsilon:0.15735328210778965\n",
      "episode: 185, total reward :-443.0, epsilon:0.15577974928671176\n",
      "episode: 186, total reward :-558.0, epsilon:0.15422195179384465\n",
      "episode: 187, total reward :-876.0, epsilon:0.1526797322759062\n",
      "episode: 188, total reward :-514.0, epsilon:0.15115293495314713\n",
      "episode: 189, total reward :-471.0, epsilon:0.14964140560361566\n",
      "episode: 190, total reward :-225.0, epsilon:0.1481449915475795\n",
      "episode: 191, total reward :-4852.0, epsilon:0.1466635416321037\n",
      "episode: 192, total reward :-1983.0, epsilon:0.14519690621578268\n",
      "episode: 193, total reward :-1496.0, epsilon:0.14374493715362485\n",
      "episode: 194, total reward :-537.0, epsilon:0.1423074877820886\n",
      "episode: 195, total reward :-882.0, epsilon:0.1408844129042677\n",
      "episode: 196, total reward :-828.0, epsilon:0.13947556877522502\n",
      "episode: 197, total reward :-850.0, epsilon:0.13808081308747278\n",
      "episode: 198, total reward :-363.0, epsilon:0.13670000495659804\n",
      "episode: 199, total reward :-1335.0, epsilon:0.13533300490703207\n",
      "episode: 200, total reward :-1223.0, epsilon:0.13397967485796175\n",
      "episode: 201, total reward :-2852.0, epsilon:0.13263987810938213\n",
      "episode: 202, total reward :-2564.0, epsilon:0.1313134793282883\n",
      "episode: 203, total reward :-2568.0, epsilon:0.13000034453500542\n",
      "episode: 204, total reward :-3305.0, epsilon:0.12870034108965536\n",
      "episode: 205, total reward :-1641.0, epsilon:0.12741333767875881\n",
      "episode: 206, total reward :-844.0, epsilon:0.12613920430197123\n",
      "episode: 207, total reward :-1071.0, epsilon:0.12487781225895152\n",
      "episode: 208, total reward :-895.0, epsilon:0.123629034136362\n",
      "episode: 209, total reward :-723.0, epsilon:0.12239274379499838\n",
      "episode: 210, total reward :-974.0, epsilon:0.1211688163570484\n",
      "episode: 211, total reward :-453.0, epsilon:0.11995712819347792\n",
      "episode: 212, total reward :-7116.0, epsilon:0.11875755691154315\n",
      "episode: 213, total reward :-1068.0, epsilon:0.11756998134242772\n",
      "episode: 214, total reward :-957.0, epsilon:0.11639428152900344\n",
      "episode: 215, total reward :-1221.0, epsilon:0.11523033871371341\n",
      "episode: 216, total reward :-406.0, epsilon:0.11407803532657627\n",
      "episode: 217, total reward :-1133.0, epsilon:0.11293725497331052\n",
      "episode: 218, total reward :-1376.0, epsilon:0.1118078824235774\n",
      "episode: 219, total reward :-542.0, epsilon:0.11068980359934164\n",
      "episode: 220, total reward :-256.0, epsilon:0.10958290556334822\n",
      "episode: 221, total reward :-498.0, epsilon:0.10848707650771475\n",
      "episode: 222, total reward :-11802.0, epsilon:0.1074022057426376\n",
      "episode: 223, total reward :-704.0, epsilon:0.10632818368521123\n",
      "episode: 224, total reward :-555.0, epsilon:0.10526490184835911\n"
     ]
    }
   ],
   "source": [
    "total_reward_episode = [0] * n_episode\n",
    "q_learning(env, dqn, n_episode, replay_size,target_update, gamma =0.9, epsilon=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
