{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prayash/.local/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "import torch\n",
    "from collections import deque\n",
    "import random \n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "env = gym.envs.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, n_state,n_action,  n_hidden=50, lr=0.05):\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_state, n_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_hidden, n_action)\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "\n",
    "        self.model_target = copy.deepcopy(self.model)\n",
    "\n",
    "    def target_predict(self, s):\n",
    "        with torch.no_grad():\n",
    "            return self.model_target(torch.Tensor(s))\n",
    "\n",
    "    def copy_target(self):\n",
    "        self.model_target.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def replay(self, memory, replay_size, gamma ):\n",
    "        if len(memory) >= replay_size:\n",
    "            replay_data = random.sample(memory, replay_size)\n",
    "            states= []\n",
    "            td_targets =[]\n",
    "            for state, action, next_state, reward, is_done in replay_data:\n",
    "                states.append(state)\n",
    "                q_values = self.predict(state).tolist()\n",
    "                if is_done:\n",
    "                    q_values[action]= reward\n",
    "                else:\n",
    "                    q_values_next = self.target_predict(next_state).detach()\n",
    "                    q_values[action] = reward + gamma*torch.max(q_values_next).item()\n",
    "\n",
    "                td_targets.append(q_values)\n",
    "\n",
    "            self.update(states, td_targets)\n",
    "    def update(self, s,y):\n",
    "        \"\"\"\n",
    "        Update the weights of the DQN given a training sample\n",
    "        @param s : state\n",
    "        @param y: target value\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = self.model(torch.Tensor(s))\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def predict(self, s ):\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_epsilon_greedy_policy(estimator, epsilon , n_action):\n",
    "    def policy_function(state):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, n_action -1 )\n",
    "\n",
    "        else :\n",
    "            q_values = estimator.predict(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "    return policy_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, estimator, n_episode, replay_size, target_update=10, gamma = 1.0, epsilon =0.1, epsilon_decay=0.99):\n",
    "    for episode in range(n_episode):\n",
    "        if episode % target_update ==0:\n",
    "            estimator.copy_target()\n",
    "        \n",
    "        policy = gen_epsilon_greedy_policy(\n",
    "            estimator, epsilon, n_action\n",
    "        )\n",
    "        state, _ = env.reset()\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, is_done,_,_ = env.step(action)\n",
    "            total_reward_episode[episode] +=reward\n",
    "            memory.append((state, action,next_state, reward,is_done))\n",
    "\n",
    "            if is_done:\n",
    "                break\n",
    "            \n",
    "            estimator.replay(memory, replay_size, gamma)\n",
    "            state = next_state\n",
    "\n",
    "        epsilon = max(epsilon* epsilon_decay, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "n_episode = 600\n",
    "last_episode = 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_options = [30,40]\n",
    "lr_options = [0.001, 0.003]\n",
    "replay_size_options = [20,25]\n",
    "target_update_options = [30, 35]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2079/3373090565.py:44: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  y_pred = self.model(torch.Tensor(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 0.001 20 30 219.085\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m memory \u001b[39m=\u001b[39m deque(maxlen\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)\n\u001b[1;32m     10\u001b[0m total_reward_episode \u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m n_episode\n\u001b[0;32m---> 11\u001b[0m q_learning(env, dqn, n_episode, replay_size, target_update, gamma\u001b[39m=\u001b[39;49m\u001b[39m.9\u001b[39;49m, epsilon\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(n_hidden, lr, replay_size, target_update, \u001b[39msum\u001b[39m(total_reward_episode[\u001b[39m-\u001b[39mlast_episode:])\u001b[39m/\u001b[39mlast_episode)\n",
      "Cell \u001b[0;32mIn [4], line 20\u001b[0m, in \u001b[0;36mq_learning\u001b[0;34m(env, estimator, n_episode, replay_size, target_update, gamma, epsilon, epsilon_decay)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[39mif\u001b[39;00m is_done:\n\u001b[1;32m     18\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     estimator\u001b[39m.\u001b[39;49mreplay(memory, replay_size, gamma)\n\u001b[1;32m     21\u001b[0m     state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m     23\u001b[0m epsilon \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(epsilon\u001b[39m*\u001b[39m epsilon_decay, \u001b[39m0.01\u001b[39m)\n",
      "Cell \u001b[0;32mIn [2], line 31\u001b[0m, in \u001b[0;36mDQN.replay\u001b[0;34m(self, memory, replay_size, gamma)\u001b[0m\n\u001b[1;32m     29\u001b[0m     q_values[action]\u001b[39m=\u001b[39m reward\n\u001b[1;32m     30\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     q_values_next \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_predict(next_state)\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     32\u001b[0m     q_values[action] \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m gamma\u001b[39m*\u001b[39mtorch\u001b[39m.\u001b[39mmax(q_values_next)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     34\u001b[0m td_targets\u001b[39m.\u001b[39mappend(q_values)\n",
      "Cell \u001b[0;32mIn [2], line 15\u001b[0m, in \u001b[0;36mDQN.target_predict\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtarget_predict\u001b[39m(\u001b[39mself\u001b[39m, s):\n\u001b[1;32m     14\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_target(torch\u001b[39m.\u001b[39;49mTensor(s))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n_hidden in n_hidden_options:\n",
    "    for lr in lr_options:\n",
    "        for replay_size in replay_size_options:\n",
    "            for target_update in target_update_options:\n",
    "                env.action_space.seed(1)\n",
    "                random.seed(1)\n",
    "                torch.manual_seed(1)\n",
    "                dqn = DQN(n_state, n_action, n_hidden, lr)\n",
    "                memory = deque(maxlen=10000)\n",
    "                total_reward_episode =[0] * n_episode\n",
    "                q_learning(env, dqn, n_episode, replay_size, target_update, gamma=.9, epsilon=1)\n",
    "                print(n_hidden, lr, replay_size, target_update, sum(total_reward_episode[-last_episode:])/last_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
