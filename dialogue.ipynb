{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_pc = torch.tensor([1, 5 , 5,3,-1, 5, 5,3,1,-2 ])\n",
    "reward_npc = torch.tensor([0,-2,-3,-1,0, -2, -3, -1,1])\n",
    "\n",
    "#index = id-1\n",
    "# action_transform\n",
    "action_transform = torch.tensor([[1,3,6,7],[2,4,6,8],[1,5,6,7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_mapping_NPC = {\n",
    "    1:\"hello witcher\",\n",
    "    2:\"hey kid!\",\n",
    "    3:\"How is it going on brave witcher lord?\",\n",
    "    4:\"Witcher can you save us from the creature thats attacking us?\",\n",
    "    5:\"Oe witcher,  defeat a monster for me.\",\n",
    "    6:\"Lord witcher can you save us from the creature that is attacking us?\",\n",
    "    7:\"We don't know what monster attacked us but it was vicious. It must have to huge as it slaughtered no less than a dozen wolves. Ripped their guts out, but left lost uneaten. Howls at nighhts. People are afraid to venture into the woods at night. Please lord witcher save us from the beast.\",\n",
    "    8:\" We don't know what monster attacked us but it was vicious. It must have to huge as it slaughtered no less than a dozen wolves. Ripped their guts out, but left lost uneaten. Howls at nighhts. People are afraid to venture into the woods at night.\",\n",
    "    9:\"No idea what monster attacked us if i knew i would be dead. It must have to huge as it slaughtered no less than a dozen wolves. Ripped their guts out, but left lost uneaten. You sure kid like you can handle it. Howls at nighhts. People are afraid to venture into the woods at night. Sure you can take care of it we dont want any more dead bodies.\"\n",
    "}\n",
    "\n",
    "\n",
    "dialogue_mapping_PC ={\n",
    "    1:\"Greetings\",\n",
    "    2:\"Fine just busy fighting creatures from Void dimension\",\n",
    "    3:\"Ok let me help you with it. What creature is it?\",\n",
    "    4:\" I m gonna help you but i need to paid well for a contract like that. What kind of creature is it\",\n",
    "    5:\"I don't have to bother with this work. farewell\",\n",
    "    6:\" Ok, sir let me help you. What kind of creature is it.\",\n",
    "    7:\"Sure it will take 500$ \",\n",
    "    8:\" Sure it will take 800$\",\n",
    "    9:\"Sure it will take 1200$\",\n",
    "    10:\"Sorry I am busy. I won't be able to do that.\"\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_in(lister):\n",
    "   \n",
    "    \n",
    "    for i in lister:\n",
    "\n",
    "        print(dialogue_mapping_PC[i], i)\n",
    "    x=input()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment_user():\n",
    "    def __init__(self):\n",
    "        self.state=(0,0) #(NPC, PC)\n",
    "        self.superState = 0\n",
    "    def PC_action(self,action_transformed):\n",
    "        if self.superState ==0:\n",
    "            return 1 \n",
    "            ## code to ask input from the user\n",
    "            # transform into the id\n",
    "\n",
    "        elif self.superState == 1:\n",
    "            if action_transformed == 3:\n",
    "                return 2\n",
    "       \n",
    "            else:\n",
    "                lister = [3,4]\n",
    "                x= user_in(lister)\n",
    "                return int(x)\n",
    "\n",
    "        elif self.superState ==2:\n",
    "            return 6\n",
    "\n",
    "        elif self.superState ==3 :\n",
    "            lister = [7,8,9,10]\n",
    "            x= user_in(lister)\n",
    "            return int(x)  \n",
    "\n",
    "        else :\n",
    "            return -1\n",
    "        \n",
    "    def return_reward(self):\n",
    "        return reward_npc[self.state[0]-1] + reward_pc[self.state[1]-1]\n",
    "\n",
    "\n",
    "    def next_state(self,action_transformed ):\n",
    "        pc_action = self.PC_action(action_transformed)\n",
    "        if pc_action ==-1:\n",
    "            self.state =(-1,-1)\n",
    "            return self.state\n",
    "        self.state = (action_transformed,pc_action)\n",
    "        if self.superState == 0:\n",
    "            \n",
    "            self.superState =1\n",
    "        elif self.superState == 1:\n",
    "\n",
    "            if action_transformed == 2:\n",
    "\n",
    "                self.superState = 2\n",
    "            elif pc_action == 6:\n",
    "                self.superState =4\n",
    "\n",
    "            else :\n",
    "                self.superState=3\n",
    "\n",
    "\n",
    "        elif self.superState ==2:\n",
    "            self.superState = 3\n",
    "\n",
    "        elif self.superState ==3:\n",
    "            self.superState = 4\n",
    "\n",
    "        else:\n",
    "            self.superState = 4\n",
    "        \n",
    "        return self.state\n",
    "        \n",
    "                           \n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# This is just a dummy environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self):\n",
    "        self.state=(0,0)\n",
    "        self.superState = 0\n",
    "    def PC_action(self,action_transformed):\n",
    "        if self.superState ==0:\n",
    "            return 1 \n",
    "            ## code to ask input from the user\n",
    "            # transform into the id\n",
    "\n",
    "        elif self.superState == 1:\n",
    "            if action_transformed == 3:\n",
    "                return 2\n",
    "            elif action_transformed ==5:\n",
    "                return -1\n",
    "            else:\n",
    "                lister = [3,4]\n",
    "                return random.choice(lister)\n",
    "\n",
    "        elif self.superState ==2:\n",
    "            return 6\n",
    "\n",
    "        elif self.superState ==3 :\n",
    "            lister = [7,8,9,10]\n",
    "            return random.choice(lister)   \n",
    "\n",
    "        else :\n",
    "            return -1\n",
    "        \n",
    "    def return_reward(self):\n",
    "        return reward_npc[self.state[0]-1] + reward_pc[self.state[1]-1]\n",
    "\n",
    "\n",
    "    def next_state(self,action_transformed ):\n",
    "        pc_action = self.PC_action(action_transformed)\n",
    "        if pc_action ==-1:\n",
    "            self.state =(-1,-1)\n",
    "            return self.state\n",
    "        self.state = (action_transformed,pc_action)\n",
    "        if self.superState == 0:\n",
    "            \n",
    "            self.superState =1\n",
    "        elif self.superState == 1:\n",
    "\n",
    "            if action_transformed == 2:\n",
    "\n",
    "                self.superState = 2\n",
    "            elif pc_action == 6:\n",
    "                self.superState =4\n",
    "                self.state = (-1,-1)\n",
    "\n",
    "            else :\n",
    "                self.superState=3\n",
    "\n",
    "\n",
    "        elif self.superState ==2:\n",
    "            self.superState = 3\n",
    "\n",
    "        elif self.superState ==3:\n",
    "            self.superState = 4\n",
    "\n",
    "        else:\n",
    "            self.superState = 4\n",
    "            self.state = (-1,-1)\n",
    "        #print(self.state[1])\n",
    "        return self.state\n",
    "        \n",
    "                           \n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# This is just a dummy environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiata a environment\n",
    "\n",
    "env = Environment_user()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, n_state, n_action, n_hidden=50, lr=0.05):\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_state,n_hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_hidden, n_action)\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr)\n",
    "\n",
    "        self.model_target = copy.deepcopy(self.model)\n",
    "\n",
    "    def target_predict(self, s):\n",
    "        with torch.no_grad():\n",
    "            return self.model_target(torch.Tensor(s))\n",
    "\n",
    "    def copy_target(self):\n",
    "        self.model_target.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def update(self, s,y):\n",
    "        y_pred = self.model(torch.Tensor(s))\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def predict(self, s):\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(s))\n",
    "\n",
    "\n",
    "            \n",
    "    def replay(self, memory, replay_size, gamma):\n",
    "        if len(memory) >= replay_size:\n",
    "            replay_data = random.sample(memory , replay_size)\n",
    "            states =[]\n",
    "            td_targets =[]\n",
    "            for state, action, next_state, reward, is_done in replay_data:\n",
    "                states.append(state)\n",
    "                q_values = self.predict(state).tolist()\n",
    "                if is_done:\n",
    "                    q_values[action] = reward\n",
    "                else:\n",
    "                    q_values_next = self.target_predict(next_state).detach()\n",
    "                    q_values[action] = reward + gamma* torch.max(q_values_next).item()\n",
    "\n",
    "                td_targets.append(q_values)\n",
    "\n",
    "            states = np.array(states)\n",
    "            self.update(states,td_targets)\n",
    "\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_epsilon_greed_policy(estimator,epsilon, n_action ):\n",
    "    def policy_function(state):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0,n_action-1)\n",
    "\n",
    "        else:\n",
    "            q_values = estimator.predict(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "    return policy_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_state = 2\n",
    "n_action = 3\n",
    "n_hidden =50\n",
    "lr = 0.01\n",
    "dqn = DQN(n_state, n_action, n_hidden, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, estimator , n_episode, replay_size, target_update=10, gamma=1.0, epsilon=0.1 , epsilon_decay=0.99):\n",
    "    for episode in range(n_episode):\n",
    "        if episode % target_update ==0 :\n",
    "            estimator.copy_target()\n",
    "        policy = gen_epsilon_greed_policy(estimator, epsilon, n_action)\n",
    "        env.state=(0,0)\n",
    "        env.superState = 0\n",
    "        state = env.state\n",
    "\n",
    "        is_done = False\n",
    "        print(state)\n",
    "\n",
    "        while not is_done:\n",
    "            \n",
    "            action = policy(state)\n",
    "            \n",
    "            transform_action = action_transform[action,env.superState]\n",
    "             \n",
    "           # print(\"npc:\", transform_action)\n",
    "            next_state = env.next_state(transform_action)\n",
    "            print(dialogue_mapping_NPC[next_state[0].item()])\n",
    "            print(dialogue_mapping_PC[next_state[1]])\n",
    "            reward =env.return_reward()\n",
    "            total_reward_episode[episode] += reward\n",
    "\n",
    "            \n",
    "\n",
    "            memory.append((state, action, next_state, reward, is_done))\n",
    "            if env.superState == 4:\n",
    "                is_done = True\n",
    "            if is_done:\n",
    "                print(\"end episode\")\n",
    "                break\n",
    "\n",
    "            estimator.replay(memory, replay_size, gamma)\n",
    "            state = next_state\n",
    "            print(state)\n",
    "\n",
    "        print('episode: {}, total reward : {}, epsilon:{}'.format(episode, total_reward_episode[episode], epsilon))\n",
    "        epsilon = max(epsilon * epsilon_decay, 0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 1000\n",
    "replay_size = 20 \n",
    "\n",
    "target_update = 10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "hello witcher\n",
      "Greetings\n",
      "(tensor(1), 1)\n",
      "Ok let me help you with it. What creature is it? 3\n",
      " I m gonna help you but i need to paid well for a contract like that. What kind of creature is it 4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [73], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m total_reward_episode \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m n_episode\n\u001b[0;32m----> 2\u001b[0m q_learning(env, dqn, n_episode, replay_size, target_update, gamma\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m, epsilon \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [71], line 20\u001b[0m, in \u001b[0;36mq_learning\u001b[0;34m(env, estimator, n_episode, replay_size, target_update, gamma, epsilon, epsilon_decay)\u001b[0m\n\u001b[1;32m     17\u001b[0m  transform_action \u001b[39m=\u001b[39m action_transform[action,env\u001b[39m.\u001b[39msuperState]\n\u001b[1;32m     19\u001b[0m \u001b[39m# print(\"npc:\", transform_action)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m  next_state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mnext_state(transform_action)\n\u001b[1;32m     21\u001b[0m  \u001b[39mprint\u001b[39m(dialogue_mapping_NPC[next_state[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitem()])\n\u001b[1;32m     22\u001b[0m  \u001b[39mprint\u001b[39m(dialogue_mapping_PC[next_state[\u001b[39m1\u001b[39m]])\n",
      "Cell \u001b[0;32mIn [46], line 36\u001b[0m, in \u001b[0;36mEnvironment_user.next_state\u001b[0;34m(self, action_transformed)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext_state\u001b[39m(\u001b[39mself\u001b[39m,action_transformed ):\n\u001b[0;32m---> 36\u001b[0m     pc_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPC_action(action_transformed)\n\u001b[1;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m pc_action \u001b[39m==\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m     38\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn [46], line 18\u001b[0m, in \u001b[0;36mEnvironment_user.PC_action\u001b[0;34m(self, action_transformed)\u001b[0m\n\u001b[1;32m     16\u001b[0m         lister \u001b[39m=\u001b[39m [\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m]\n\u001b[1;32m     17\u001b[0m         x\u001b[39m=\u001b[39m user_in(lister)\n\u001b[0;32m---> 18\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39;49m(x)\n\u001b[1;32m     20\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuperState \u001b[39m==\u001b[39m\u001b[39m2\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m6\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "total_reward_episode = [0] * n_episode\n",
    "q_learning(env, dqn, n_episode, replay_size, target_update, gamma=0.9, epsilon = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
